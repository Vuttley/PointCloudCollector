{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa28d4-3e9f-41d1-a284-2942e685c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877bc666-6954-4a26-8363-031a8dc5c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"Biblioteca 'tqdm' não encontrada. Para uma barra de progresso, instale com: pip install tqdm\")\n",
    "    def tqdm(iterator, *args, **kwargs):\n",
    "        return iterator\n",
    "\n",
    "def gerar_ply_otimizado(df, nome_arquivo=\"nuvem_de_pontos_otimizada.ply\"):\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"DataFrame vazio. Nenhum arquivo PLY será gerado.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Iniciando geração otimizada de PLY para {len(df)} pontos...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    distancias_m = df['distancia_lidar'].to_numpy() / 1000.0\n",
    "    angulos_lidar_rad = np.deg2rad(df['angulo_lidar'].to_numpy())\n",
    "    pitches_rad = -np.deg2rad(df['pitch'].to_numpy())\n",
    "    yaws_rad = -np.deg2rad(df['yaw'].to_numpy())\n",
    "    intensidades = df['intensidade_lidar'].to_numpy()\n",
    "\n",
    "    normalizer = colors.Normalize(vmin=np.min(intensidades), vmax=np.max(intensidades))\n",
    "    colormap = plt.get_cmap('viridis')\n",
    "    cores_normalizadas = normalizer(intensidades)\n",
    "    cores_rgba = colormap(cores_normalizadas)\n",
    "    cores_rgb_uchar = (cores_rgba[:, :3] * 255).astype(np.uint8)\n",
    "\n",
    "    d_cabeca_para_sensor = np.array([0, 0, 0.082])\n",
    "    p_no_sensor = np.zeros((len(df), 3))\n",
    "    p_no_sensor[:, 1] = distancias_m\n",
    "\n",
    "    cos_a = np.cos(angulos_lidar_rad)\n",
    "    sin_a = np.sin(angulos_lidar_rad)\n",
    "    \n",
    "    p_laser_rotacionado = np.zeros_like(p_no_sensor)\n",
    "    p_laser_rotacionado[:, 0] = -distancias_m * sin_a\n",
    "    p_laser_rotacionado[:, 1] =  distancias_m * cos_a\n",
    "    p_laser_rotacionado[:, 2] =  0\n",
    "\n",
    "    vetor_total_na_cabeca = p_laser_rotacionado + d_cabeca_para_sensor\n",
    "\n",
    "    cos_p = np.cos(pitches_rad)\n",
    "    sin_p = np.sin(pitches_rad)\n",
    "    \n",
    "    x_cabeca = vetor_total_na_cabeca[:, 0]\n",
    "    y_cabeca = vetor_total_na_cabeca[:, 1]\n",
    "    z_cabeca = vetor_total_na_cabeca[:, 2]\n",
    "    \n",
    "    vetor_apos_pitch = np.zeros_like(vetor_total_na_cabeca)\n",
    "    vetor_apos_pitch[:, 0] =  x_cabeca * cos_p + z_cabeca * sin_p\n",
    "    vetor_apos_pitch[:, 1] =  y_cabeca\n",
    "    vetor_apos_pitch[:, 2] = -x_cabeca * sin_p + z_cabeca * cos_p\n",
    "\n",
    "    cos_y = np.cos(yaws_rad)\n",
    "    sin_y = np.sin(yaws_rad)\n",
    "\n",
    "    x_pitch = vetor_apos_pitch[:, 0]\n",
    "    y_pitch = vetor_apos_pitch[:, 1]\n",
    "    z_pitch = vetor_apos_pitch[:, 2]\n",
    "\n",
    "    p_final_no_mundo = np.zeros_like(vetor_apos_pitch)\n",
    "    p_final_no_mundo[:, 0] = x_pitch * cos_y - y_pitch * sin_y\n",
    "    p_final_no_mundo[:, 1] = x_pitch * sin_y + y_pitch * cos_y\n",
    "    p_final_no_mundo[:, 2] = z_pitch\n",
    "\n",
    "    print(\"Cálculos finalizados. Escrevendo arquivo PLY...\")\n",
    "    pontos_com_cores = np.hstack((p_final_no_mundo, cores_rgb_uchar))\n",
    "    \n",
    "    num_pontos = len(df)\n",
    "    header = [\n",
    "        \"ply\", \"format ascii 1.0\", f\"element vertex {num_pontos}\",\n",
    "        \"property float x\", \"property float y\", \"property float z\",\n",
    "        \"property uchar red\", \"property uchar green\", \"property uchar blue\",\n",
    "        \"end_header\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "\n",
    "        np.savetxt(\n",
    "            nome_arquivo, pontos_com_cores,\n",
    "            fmt=\"%.4f %.4f %.4f %d %d %d\",\n",
    "            header=\"\\n\".join(header),\n",
    "            comments=''\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        print(f\"Sucesso! Arquivo '{os.path.basename(str(nome_arquivo))}' salvo em {end_time - start_time:.2f} segundos.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO: Falha ao salvar o arquivo PLY: {e}\")\n",
    "\n",
    "def load_raw_data(filename):\n",
    "    \n",
    "    data = []\n",
    "    print(f\"Carregando dados de '{os.path.basename(str(filename))}'...\")\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Aviso: linha malformada ignorada em {os.path.basename(str(filename))}\")\n",
    "    print(f\"Carregados {len(data)} registros.\")\n",
    "    return data\n",
    "\n",
    "def process_and_filter_data(lidar_data, imu_data, output_filename, percentile_to_keep=98.0):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not lidar_data or not imu_data or len(imu_data) < 5:\n",
    "        print(\"ERRO: Dados insuficientes para processamento.\")\n",
    "        return False\n",
    "\n",
    "    lidar_timestamps_ns = np.array([p['timestamp_ns'] for p in lidar_data])\n",
    "    lidar_distances = np.array([p['distance'] for p in lidar_data])\n",
    "    lidar_angles = np.array([p['angle'] for p in lidar_data])\n",
    "    lidar_intensities = np.array([p['intensity'] for p in lidar_data])\n",
    "\n",
    "    imu_timestamps_ns = np.array([m['timestamp_ns'] for m in imu_data])\n",
    "    imu_pitches = np.array([m['data']['pitch'] for m in imu_data])\n",
    "    imu_yaws_unwrapped = np.unwrap([m['data']['yaw'] for m in imu_data], period=360)\n",
    "\n",
    "    indices = np.searchsorted(imu_timestamps_ns, lidar_timestamps_ns, side='right')\n",
    "    valid_indices_mask = (indices > 0) & (indices < len(imu_timestamps_ns))\n",
    "    \n",
    "    lidar_timestamps_ns = lidar_timestamps_ns[valid_indices_mask]\n",
    "    lidar_distances = lidar_distances[valid_indices_mask]\n",
    "    lidar_angles = lidar_angles[valid_indices_mask]\n",
    "    lidar_intensities = lidar_intensities[valid_indices_mask]\n",
    "    print(f\"   {len(lidar_timestamps_ns)} pontos de LiDAR com timestamps válidos.\")\n",
    "    \n",
    "    smoothing_factor = np.cbrt(len(imu_timestamps_ns))\n",
    "    spline_pitch = UnivariateSpline(imu_timestamps_ns, imu_pitches, k=3, s=smoothing_factor)\n",
    "    spline_yaw = UnivariateSpline(imu_timestamps_ns, imu_yaws_unwrapped, k=3, s=smoothing_factor)\n",
    "    final_pitches = spline_pitch(lidar_timestamps_ns)\n",
    "    final_yaws = spline_yaw(lidar_timestamps_ns)\n",
    "    final_yaws = np.array([np.mean(final_yaws)]*len(final_yaws))\n",
    "    final_rolls = np.zeros_like(lidar_timestamps_ns)\n",
    "    \n",
    "    print(f\"Filtrando pontos instáveis (mantendo {percentile_to_keep}% dos mais estáveis)...\")\n",
    "    spline_pitch_acc = spline_pitch.derivative(n=2)    \n",
    "    pitch_accels = spline_pitch_acc(lidar_timestamps_ns)\n",
    "    motion_instability = np.abs(pitch_accels)\n",
    "    \n",
    "    if motion_instability.size > 0:\n",
    "        instability_threshold = np.percentile(motion_instability, percentile_to_keep)\n",
    "        valid_motion_mask = motion_instability <= instability_threshold\n",
    "        num_original = len(lidar_timestamps_ns)\n",
    "        \n",
    "        lidar_timestamps_ns = lidar_timestamps_ns[valid_motion_mask]\n",
    "        lidar_distances = lidar_distances[valid_motion_mask]\n",
    "        lidar_angles = lidar_angles[valid_motion_mask]\n",
    "        lidar_intensities = lidar_intensities[valid_motion_mask]\n",
    "        final_pitches = final_pitches[valid_motion_mask]\n",
    "        final_yaws = final_yaws[valid_motion_mask]\n",
    "        final_rolls = final_rolls[valid_motion_mask]\n",
    "        \n",
    "        num_final = len(lidar_timestamps_ns)\n",
    "        num_descartados = num_original - num_final\n",
    "        percent_descartados = (num_descartados / num_original) * 100 if num_original > 0 else 0\n",
    "        print(f\"   Filtro de movimento descartou {num_descartados} pontos ({percent_descartados:.2f}%).\")\n",
    "    else:\n",
    "        print(\"   Nenhum ponto para filtrar.\")\n",
    "\n",
    "    print(f\"Salvando {len(lidar_timestamps_ns)} pontos finais em '{os.path.basename(str(output_filename))}'...\")\n",
    "    fieldnames = [\n",
    "        'distancia_lidar', 'angulo_lidar', 'intensidade_lidar',\n",
    "        'roll', 'pitch', 'yaw', 'timestamp'\n",
    "    ]\n",
    "    try:\n",
    "        with open(output_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for i in range(len(lidar_timestamps_ns)):\n",
    "                writer.writerow({\n",
    "                    'distancia_lidar': lidar_distances[i], 'angulo_lidar': lidar_angles[i],\n",
    "                    'intensidade_lidar': lidar_intensities[i], 'roll': final_rolls[i],\n",
    "                    'pitch': final_pitches[i], 'yaw': (final_yaws[i] + 180) % 360 - 180,\n",
    "                    'timestamp': lidar_timestamps_ns[i],\n",
    "                })\n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"Sucesso! Processamento concluído em {processing_time:.2f} segundos.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO: Falha ao salvar o arquivo CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "def remove_statistical_outlier_variable(pcd, df,\n",
    "                                           distance_col='distancia_lidar', angle_col='angulo_lidar',\n",
    "                                           neighbor_density_factor=200000.0, std_ratio=2.0,\n",
    "                                           min_neighbors=10, max_neighbors=200):\n",
    "    \n",
    "    if len(pcd.points) != len(df):\n",
    "        raise ValueError(\"A Nuvem de Pontos e o DataFrame devem ter o mesmo número de pontos.\")\n",
    "\n",
    "    if len(pcd.points) == 0:\n",
    "        return pcd, []\n",
    "\n",
    "    points = np.asarray(pcd.points)\n",
    "    tree = cKDTree(points)\n",
    "\n",
    "    distances = df[distance_col].to_numpy(dtype=float)\n",
    "    angles = np.abs(np.deg2rad(df[angle_col].to_numpy(dtype=float)))\n",
    "    \n",
    "    distances[distances < 0.01] = 0.01\n",
    "    angles[angles < 0.01] = 0.01\n",
    "\n",
    "    k_values = neighbor_density_factor / (distances * angles)\n",
    "    k_values = np.clip(k_values, min_neighbors, max_neighbors).astype(int)\n",
    "\n",
    "    avg_neighbor_distances = np.zeros(len(points))\n",
    "\n",
    "    for i in tqdm(range(len(points)), desc=\"Analisando vizinhanças dos pontos\"):\n",
    "        k = k_values[i]\n",
    "        neighbor_dists, _ = tree.query(points[i], k=k+1, p=2)\n",
    "        avg_neighbor_distances[i] = np.mean(neighbor_dists[1:])\n",
    "\n",
    "    mean_of_avg_dists = np.mean(avg_neighbor_distances)\n",
    "    std_dev_of_avg_dists = np.std(avg_neighbor_distances)\n",
    "    distance_threshold = mean_of_avg_dists + std_ratio * std_dev_of_avg_dists\n",
    "    inlier_indices = np.where(avg_neighbor_distances <= distance_threshold)[0]\n",
    "    \n",
    "    pcd_cleaned = pcd.select_by_index(inlier_indices)\n",
    "    \n",
    "    print(f\"Filtragem concluída. Limiar de distância: {distance_threshold:.4f}. Pontos mantidos: {len(inlier_indices)} de {len(points)}\")\n",
    "\n",
    "    return pcd_cleaned, inlier_indices\n",
    "\n",
    "def process_scan_pair(base_path: Path, scan_id: str):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"INICIANDO PROCESSAMENTO PARA O SCAN ID: {scan_id}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    imu_file = base_path / f\"raw_imu_pan_{scan_id}.txt\"\n",
    "    lidar_file = base_path / f\"raw_lidar_pan_{scan_id}.txt\"\n",
    "    \n",
    "    temp_dir = base_path / \"processamento_temporario\"\n",
    "    if not temp_dir.exists():\n",
    "        temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    output_csv = temp_dir / f\"{scan_id}_processado.csv\"\n",
    "    intermediate_ply = temp_dir / f\"{scan_id}_intermediario.ply\"\n",
    "    final_output_ply = base_path / f\"{scan_id}.ply\"\n",
    "\n",
    "    PERCENTIL_A_MANTER = 40.0\n",
    "    NEIGHBOR_DENSITY_FACTOR = 100.0 \n",
    "    STD_RATIO = 1.7\n",
    "    MIN_NEIGHBORS = 200\n",
    "    MAX_NEIGHBORS = 10000\n",
    "    VOXEL_SIZE = 0.001\n",
    "\n",
    "    try:\n",
    "        imu_data = load_raw_data(imu_file)\n",
    "        lidar_data = load_raw_data(lidar_file)\n",
    "\n",
    "        if not process_and_filter_data(lidar_data, imu_data, output_csv, percentile_to_keep=PERCENTIL_A_MANTER):\n",
    "            raise Exception(\"Falha na etapa de processamento e filtragem de dados.\")\n",
    "\n",
    "        df = pd.read_csv(output_csv)\n",
    "        if df.empty:\n",
    "            print(f\"AVISO: Nenhum ponto de dados foi gerado para o scan {scan_id}. Pulando para o próximo.\")\n",
    "            return\n",
    "\n",
    "        gerar_ply_otimizado(df, nome_arquivo=intermediate_ply)\n",
    "\n",
    "        pcd = o3d.io.read_point_cloud(str(intermediate_ply))\n",
    "        if not pcd.has_points():\n",
    "             raise Exception(f\"Falha ao ler o arquivo PLY intermediário ou o arquivo está vazio: {intermediate_ply}\")\n",
    "\n",
    "        pcd_downsampled = pcd.voxel_down_sample(VOXEL_SIZE)\n",
    "\n",
    "        print(\"Mapeando pontos da nuvem reduzida para o DataFrame original...\")\n",
    "        pcd_tree = o3d.geometry.KDTreeFlann(pcd)\n",
    "        indices_map = []\n",
    "        for point in np.asarray(pcd_downsampled.points):\n",
    "            [_, idx, _] = pcd_tree.search_knn_vector_3d(point, 1)\n",
    "            indices_map.append(idx[0])\n",
    "\n",
    "        df_downsampled = df.iloc[indices_map].reset_index(drop=True)\n",
    "        print(f\"Mapeamento concluído. A nuvem reduzida tem {len(df_downsampled)} pontos.\")\n",
    "\n",
    "        print(\"Iniciando remoção de outliers com densidade variável...\")\n",
    "        pcd_cleaned, ind = remove_statistical_outlier_variable(\n",
    "            pcd=pcd_downsampled,\n",
    "            df=df_downsampled,\n",
    "            neighbor_density_factor=NEIGHBOR_DENSITY_FACTOR,\n",
    "            std_ratio=STD_RATIO,\n",
    "            min_neighbors=MIN_NEIGHBORS,\n",
    "            max_neighbors=MAX_NEIGHBORS\n",
    "        )\n",
    "\n",
    "        radius_normal = VOXEL_SIZE * 2\n",
    "        pcd_cleaned.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "        o3d.io.write_point_cloud(str(final_output_ply), pcd_cleaned, write_ascii=True)\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"SUCESSO! Nuvem de pontos final salva como '{final_output_ply.name}'\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"!\"*80)\n",
    "        print(f\"ERRO no processamento do scan ID {scan_id}: {e}\")\n",
    "        print(\"!\"*80 + \"\\n\")\n",
    "\n",
    "    finally:\n",
    "        if temp_dir.exists():\n",
    "            print(f\"Limpando arquivos temporários para o scan ID {scan_id}...\")\n",
    "            shutil.rmtree(temp_dir)\n",
    "            print(\"Limpeza concluída.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71f6b1-32b4-428e-820d-2cf7de6345e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    base_path = Path('/scan_results_2025-08-14_15-30-41/')\n",
    "    if not base_path.is_dir():\n",
    "        print(f\"ERRO: O diretório especificado não existe: '{base_path}'\")\n",
    "        return\n",
    "\n",
    "    lidar_files = list(base_path.glob(\"raw_lidar_pan_*.txt\"))\n",
    "    if not lidar_files:\n",
    "        print(f\"Nenhum arquivo 'raw_lidar_pan_*.txt' encontrado em '{base_path}'\")\n",
    "        return\n",
    "\n",
    "    scan_ids = []\n",
    "    for f in lidar_files:\n",
    "        match = re.search(r'raw_lidar_pan_(\\d+)\\.txt', f.name)\n",
    "        if match:\n",
    "            scan_ids.append(match.group(1))\n",
    "\n",
    "    scan_ids.sort(key=int)\n",
    "    \n",
    "    print(f\"Encontrados {len(scan_ids)} scans para processar: {', '.join(scan_ids)}\")\n",
    "\n",
    "    for scan_id in scan_ids:\n",
    "        imu_file = base_path / f\"raw_imu_pan_{scan_id}.txt\"\n",
    "        if imu_file.exists():\n",
    "            process_scan_pair(base_path, scan_id)\n",
    "        else:\n",
    "            print(f\"\\nAVISO: Arquivo LIDAR 'raw_lidar_pan_{scan_id}.txt' encontrado, mas o arquivo IMU correspondente não. Pulando este par.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
